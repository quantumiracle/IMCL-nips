
# IMCL-nips
Recently our team is working on the Nips2017: *learn to run* competition with Reinforcement Learning.  
Here are the techniques we use in our RL practice. 

# parallel computing with multithread and multiprocessing  
There is a balance between the actor and critic network although it is an off-policy method for DDPG.
# the reward setting 
Q value without scaling will be larger in step 100 than in step 900 in the same episode. Because <a href="https://www.codecogs.com/eqnedit.php?latex=Q_{(s)}=R&plus;\gamma&space;*Q_{(s{}')}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_{(s)}=R&plus;\gamma&space;*Q_{(s{}')}" title="Q_{(s)}=R+\gamma *Q_{(s{}')}" /></a>  
It is the Q value that trasmits the reward information from the subsequent steps to the former ones. It's also the only way for the agent to see from a globle view of the episode with such reward per step setting in this task. The more steps the agent takes, generally, the larger Q value of states in its former steps will be. So as to be consistent with our goal-a farthest running agent we want. But does this make sense? Whether or not the Q value for states with similar steps order should be scaled?  
# preprocessing of observations
The observations in this specific task can be extended to the vector representation of states in the Markov process, i.e. what variables should we use to represent the states sufficiently and efficiently. 'sufficiently' means the total number of variables is large enough to express all the information we need. 'efficiently' means there should be as little as possible redundant or overlapped information in our representation vector of states. More importantly, *there is a balance between the complexity of neural network* we used, appearing as #layers and #nodes or #parameters in the network, *and the complexity of task* we're dealing with, comprising the amount of information we need to use like #dimensions of states representation vector and complexity of function mapping these inputs to outputs of the network. When it comes to reaching to this balance, the power of computation is a intermediate part. Do you have time long enough to wait for a large enough neural network to converge to relatively simple mapping function with a process of learning to set most of its weights parameters to be near 0. So the choice of a network of proper size matters, without mentioning the fact that it is usually too hard for a relatively too large network to map a simple function. On the other side, a small network with less parameters may not have capability to express the complexity of the mapping function. Preprocessing the observations( representation of states or say inputs) is a key point in a learning task in order to cut the size of network, exclude the redundant information and stress important(i.e.most expressive) information or variables, of course, via prior experience of human. There are also other *balances problem* in RL process.
