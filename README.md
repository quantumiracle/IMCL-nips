
# IMCL-nips
Recently our team is working on the Nips2017: *learn to run* competition with Reinforcement Learning.  
Here are the techniques we use in our RL practice. 

# parallel computing with multithread and multiprocessing  
There is a balance between the actor and critic network although it is an off-policy method for DDPG. One key point to show that parallelization will affect the performance of the model: considering that the interaction betweent tha agent and env is mainly CPU-consumption while the back propagation (i.e. network training) process is mainly GPU-consumption, a code with multiprocessing forward (interaction with env) process will generate samples quickly and fill in the memory soon but with relatively similar policy, which means the samples in the memory used for backward (back propagation network training) process is relatively similar. But a code with fewer processes for forward will make the samples in memory relatively discrepant. This difference caused by the unbalance of using parallelization (or say it's a internal question in DDPG because of off-policy method) results in a prominent discrepancy in learning performance in practice.
# the reward setting 
Q value without scaling will be larger in step 100 than in step 900 in the same episode. Because <a href="https://www.codecogs.com/eqnedit.php?latex=Q_{(s)}=R&plus;\gamma&space;*Q_{(s{}')}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_{(s)}=R&plus;\gamma&space;*Q_{(s{}')}" title="Q_{(s)}=R+\gamma *Q_{(s{}')}" /></a>  
It is the Q value that trasmits the reward information from the subsequent steps to the former ones. It's also the only way for the agent to see from a globle view of the episode with such reward per step setting in this task. The more steps the agent takes, generally, the larger Q value of states in its former steps will be. So as to be consistent with our goal-a farthest running agent we want. But does this make sense? Whether or not the Q value for states with similar steps order should be scaled?  
# preprocessing of observations
The observations in this specific task can be extended to the vector representation of states in the Markov process, i.e. what variables should we use to represent the states sufficiently and efficiently. 'sufficiently' means the total number of variables is large enough to express all the information we need. 'efficiently' means there should be as little as possible redundant or overlapped information in our representation vector of states.  
More importantly, *there is a balance between the complexity of neural network* we used, appearing as #layers and #nodes or #parameters in the network, *and the complexity of task* we're dealing with, comprising the amount of information we need to use like #dimensions of states representation vector and complexity of function mapping these inputs to outputs of the network. When it comes to reaching to this balance, the power of computation is a intermediate part. A relatively larger network needs a relatively longer time to converge and see if it works. Do you have time long enough to wait for a large enough neural network to converge to relatively simple mapping function with a process of learning to set most of its weights parameters to be near 0. So the choice of a network of proper size matters, without mentioning the fact that it is usually too hard for a relatively too large network to map a simple function. On the other side, a small network with less parameters may not have capability to express the complexity of the mapping function. Preprocessing the observations( representation of states or say inputs) is a key point in a learning task in order to cut the size of network, exclude the redundant information and stress important(i.e.most expressive) information or variables, of course, via prior experience of human. There are also other *balances problem or tradeoff* in RL process.
# tradeoff 
# an interesting question
In the Run task there are 2 limits to end a episode of process, 1)episode #steps maximum 1000; 2)the y-axis value of pelvis lower than 0.65. But in order to have a better running gesture, we set the second limit to be 0.5. So that the skeleton could keep its center of gravity lower in the running process, which shows up to be a more biological way. As a result of this process, the reward we get is actually false-it is higher than it should be, i.e. it is a biased-reward.  
**the bias problem**: Generally, 'unbiased' means it is a accurate estimated model. The origin Q formula in DDPG is <a href="https://www.codecogs.com/eqnedit.php?latex=Q_{(s)}=R&plus;\gamma&space;*max(Q_{(s{}')})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_{(s)}=R&plus;\gamma&space;*max(Q_{(s{}')})" title="Q_{(s)}=R+\gamma *max(Q_{(s{}')})" /></a> while in code we actually use the output of a target_critic network i.e. <a href="https://www.codecogs.com/eqnedit.php?latex=Q_{(s{}')}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_{(s{}')}" title="Q_{(s{}')}" /></a> to replace the <a href="https://www.codecogs.com/eqnedit.php?latex=max(Q_{(s{}')})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?max(Q_{(s{}')})" title="max(Q_{(s{}')})" /></a>. However, both of these two ways are biased estimation for learning which means the learning result could be near the ideal model but equal to the ideal one only by randomness (means almost couldn't happen). While the unbiased estimation should be <a href="https://www.codecogs.com/eqnedit.php?latex=Q_{(s)}=R&plus;\gamma&space;*V_{(s{}')}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q_{(s)}=R&plus;\gamma&space;*V_{(s{}')}" title="Q_{(s)}=R+\gamma *V_{(s{}')}" /></a>. The reason why we use the biased way instead of the unbiased one is because of the variance of statistic value of a state i.e.<a href="https://www.codecogs.com/eqnedit.php?latex=V_{(s{}')}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{(s{}')}" title="V_{(s{}')}" /></a>. And there is a tradeoff between the variance and the accuracy of estimation. As is showed in [paper](https://arxiv.org/pdf/1611.02247.pdf).  
Let's get back to the 0.65->0.5 limit setting problem. The 'biased-reward' means the final model we get would not be an ideal solution for the origin task settings. The agent should have received a 'DONE' signal to end the episode with pelvis lower than 0.65 but it keeps moving when it is higher than 0.5. After this pretraining, how could we let the agent learn to avoid its improper behavior to make pelvis lower than 0.65 but keep its good gesture that have learned? Possible solutions are as follows: 1).Retrain the agent with a limit of 0.65 based on the current 0.5 fitting model.(We use its saved weights but not its saved memory) 2). Retrain the agent with a reward penalty (for example, a discount factor r) when its pelvis is lower than 0.65. For the former solution, the r factor equals to 0 while the latter one is a non-zero number. Besides, the former one end the episode  so there are no more rewards after this 'DONE' step while the latter one maintain the rewards 'back propagation' through the Q formula mentioned above. Intuitively, the one with a r factor reward penalty influence the agent less than the completely retrained one therefore may keep its gesture in a better way.   
# the batch size problem in RL
John Schullman has told me "the larger batch size you use (in DDPG), the better performance you will get".However, in our experiments it doen't show up like that. The batch size is actually associated with the balance of GPU and CPU consumption during the learning process. If your GPU power is well above CPU (the sample processes always SLEEP and train process always RUN), then you could expand the batch size to make your GPU work harder. I think in this situation the larger batch size the better performance you will get. On the contrary, if your GPU is over used, a larger batch size will only sleep more sample processes and therefore make sample efficiency low. The total learning speed will slow down prominently as a result of that, which is really unworthy if the model is too time-consuming.
Â 
